# -*- coding: utf-8 -*-
"""Kopie von bagofwords.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16T0YQtZSlDasJTvhfPaSmLpTGdxfOwyD
"""

import nltk
nltk.download('twitter_samples')
nltk.download('stopwords')

import pandas as pd
import numpy as np
import DummyClassifier
from sklearn.feature_extraction.text import CountVectorizer

importdf=pd.read_csv('data/trainingssets/all_emoji_tweets_15_11_18_7_labels_excluded.csv', sep =';', usecols=['tweet_full_text', 'target'])
importdf = importdf.dropna()
our_tweets=importdf['tweet_full_text'].astype(str).values.tolist()
our_targets = importdf['target'].astype(str).values.tolist()
our_tweets

"""– Remove stock market tickers like $GE
– Remove retweet text “RT”
– Remove hyperlinks
– Remove hashtags (only the hashtag # and not the word)
– Remove stop words like a, and, the, is, are, etc.
– Remove emoticons like :), :D, :(, :-), etc.
– Remove punctuation like full-stop, comma, exclamation sign, etc.
– Convert words to Stem/Base words using Porter Stemming Algorithm. E.g. words like ‘working’, ‘works’, and ‘worked’ will be converted to their base/stem word “work”.
"""

import string
import re
 
from nltk.corpus import stopwords 
stopwords_german = stopwords.words('german')
 
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer('german')
 
from nltk.tokenize import TweetTokenizer
 
# Happy Emoticons
emoticons_happy = set([
    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',
    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',
    '=-3', '=3', ':-))', ":'-)", ":')", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',
    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',
    '<3'
    ])
 
# Sad Emoticons
emoticons_sad = set([
    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',
    ':-[', ':-<', '=\\', '=/', '>:(', ':(', '>.<', ":'-(", ":'(", ':\\', ':-c',
    ':c', ':{', '>:\\', ';('
    ])
 
# all emoticons (happy + sad)
emoticons = emoticons_happy.union(emoticons_sad)
 
def clean_tweets(tweet):
    # remove stock market tickers like $GE
    tweet = re.sub(r'\$\w*', '', tweet)
 
    # remove old style retweet text "RT"
    tweet = re.sub(r'^RT[\s]+', '', tweet)
 
    # remove hyperlinks
    tweet = re.sub(r'https?:\/\/[^\s]*', '', tweet)
    
    # remove hashtags
    # only removing the hash # sign from the word
    tweet = re.sub(r'#', '', tweet)
    
    # replace years with 'ayearzzz'-Token
    tweet = re.sub(r'([1-2][0-9]{3})', r'ayearzzz', tweet)
    
    # replace numbers with 'anumberzzz'-Token, only numbers outside of words
    tweet = re.sub(r'(?<![0-9a-zA-Z])[0-9]+(?![0-9a-zA-Z])', r'anumberzzz', tweet)
 
    # tokenize tweets
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
    tweet_tokens = tokenizer.tokenize(tweet)
 
    tweets_clean = []    
    for word in tweet_tokens:
        if (word not in stopwords_german and # remove stopwords
              word not in emoticons and # remove emoticons
                word not in string.punctuation): # remove punctuation
            #tweets_clean.append(word)
            stem_word = stemmer.stem(word) # stemming word
            tweets_clean.append(stem_word)
    tweets_clean=" ".join(tweets_clean)
    
    # remove numbers that were pulled out of words by tokenizer
    tweets_clean = re.sub(r'(?<![0-9a-zA-Z])[0-9]+(?![0-9a-zA-Z])', r'', tweets_clean)
    
    return tweets_clean
 
custom_tweet = "RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np"
 
# print cleaned tweet
print (clean_tweets(custom_tweet))

y=[]
emoji_cnt = {}
for i in range(len(our_targets)):
    #Only use first emoji per tweet for now
    target_emoji = our_targets[i].split(',')[0]
    if target_emoji not in emoji_cnt:
        emoji_cnt[target_emoji] = 0
    emoji_cnt[target_emoji] += 1
    y.append(target_emoji)


# sorted_by_value = sorted(emoji_cnt.items(), key=lambda kv: kv[1])
# sorted_by_value.reverse()

# print(np.mean(sorted_by_value[0][0] == np.array(y)))

corpus=[]
for i in range(len(our_tweets)):
    corpus.append(clean_tweets(our_tweets[i]))
# corpus



# y

"""- replace numbers with "a number"-token?
- filter, that deletes non-latin letters?
    - might be hackfixed by min_df-parameter
- spellingfixes?
    - might be hackfixed by min_df-parameter
        -very inelegant, because all misspelled tokens are simply discarded
- do bold font/ different typefaces just have different asci characters?
    - might be hackfixed by min_df-parameter
-
"""

from sklearn.model_selection import train_test_split
vectorizer = CountVectorizer(min_df=60)
X = vectorizer.fit_transform(corpus)
#print(vectorizer.get_feature_names())
#print(len(vectorizer.get_feature_names()))
#print(X.toarray())  
print(X.shape)
print(len(y))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)

# clf = DummyClassifier(sorted_by_value[0][0]).fit(X, y)
# predicted = clf.predict(X_test)
# print(np.mean(predicted == y_test))

from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB().fit(X, y)
predicted = clf.predict(X_test)
print(np.mean(predicted == y_test))

# from sklearn.pipeline import Pipeline
# from sklearn.linear_model import SGDClassifier
# from sklearn.feature_extraction.text import TfidfTransformer
# text_clf = Pipeline([('vect', CountVectorizer()),
#                       ('tfidf', TfidfTransformer()),
#                       ('clf', SGDClassifier(loss='hinge', penalty='l2',
#                                             alpha=1e-3, random_state=42,
#                                             max_iter=5, tol=None)),
# ])
# text_clf.fit(X_train, y_train)
# predicted = text_clf.predict(X_test)
# np.mean(predicted == y_test)
